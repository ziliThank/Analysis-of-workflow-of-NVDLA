# Analysis-of-workflow-of-NVDLA
Source code reading
===================
## NVDLA Compiler
### Default parameters setting 

  ```
  Usage: ./nvdla_compiler [options] --prototxt <prototxt_file> --caffemodel <caffemodel_file>
  where options include:
  -h                                              print this help message
  -P                                              project
  -i <inputpath>                                  inputPath
  -o <outputpath>                                 outputs wisdom files in 'outputpath' directory
  -t                                              testname
  --prototxt                                      prototxt file
  --caffemodel
  --cachemodel
  --profileName
  --profilecfg                                    profile from file
  --profile                                       computation profile Name (default: fast-math)
  --cprecision <fp16|int8>                        compute precision (default: int8)
  --configtarget <nv_full|nv_large|nv_small>      target platform (default: nv_full)
  --calibtable <int8 calibration table>           calibration table for INT8 networks (default: 0.00787)
  --quantizationMode <per-kernel|per-filter>      quantization mode for INT8 (default: NONE)
  --batch                                         batch size (default: 0)
  --informat <ncxhwx|nchw|nhwc>                   input data format (default: nhwc)
  ```

### Workflow of Compiler

1. important structures
  
2. Set up parameters

3. launchTestï¼ˆ)
  * testSetup
    1. clear wisdom file if any exist 
    2. Initiaize TestInfor
  * parseAndCompile
    1. Create new wisdom
    2. Parse
       
        nvdla::caffe::ICaffeParser* parser = nvdla::caffe::createCaffeParser();
        b = parser->parse(caffePrototxtFile.c_str(), caffeModelFile.c_str(), network);
       ```c++
       static NvDlaError parseCaffeNetwork(const TestAppArgs* appArgs, TestInfo* i){
          const IBlobNameToTensor* CaffeParser::parse(const char*, const char*, INetwork *){
          
          
          }
          marking the network's outputs;
          parsing and setting tensor scales according to computation precision;
          attaching parsed network to the wisdom;
        
       }
       
       ```
       
4. Compile

